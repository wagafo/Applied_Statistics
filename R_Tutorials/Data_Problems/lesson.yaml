- Class: meta
  Course: R Tutorials
  Lesson: Data Problems
  Author: Walter Garcia-Fontes
  Type: Standard
  Organization: Universitat Pompeu Fabra
  Version: 1.0.0

- Class: text
  Output: In this lesson we will show different tools to perform a
   diagnosis for regressions. When you want to use regression analysis
   it is very important that you check that the main assumptions for
   the regression hold for your data. If the assumptions are violated,
   then the regression results may be completely wrong.

- Class: cmd_question
  Output: In this lesson we will use the "mtcars" data set which is
   avaliable in the "datasets" package. Check the contents of this
   data set using the str() function.
  CorrectAnswer: str(mtcars)
  AnswerTests: omnitest(correctExpr='str(mtcars)')
  Hint: Type "str(mtcars)" at the R prompt to see the contents of the
   mtcars data set.

- Class: cmd_question
  Output: We will fit a multiple regression with this data set. We
   will regress mpg (gas consumption measured by miles per gallon) on
   disp (displacement), hp (gross horsepower), wt (in pounds) and
   drat (rear axle ratio). This example is for exposition only. We
   will ignore the fact that this may not be a great way of modeling
   the this particular set of data! Use now the lm() function to
   fit this regression, and save the result in an object called "fit".
  CorrectAnswer: fit <- lm(mpg~disp+hp+wt+drat, data=mtcars)
  AnswerTests: omnitest(correctExpr='fit <- lm(mpg~disp+hp+wt+drat, data=mtcars)')
  Hint: Type "fit <- lm(mpg~disp+hp+wt+drat, data=mtcars) " at the R
   prompt to fit this regression.

- Class: cmd_question
  Output: Enter now "fit" at the R command prompt to see the results
   of the regression.
  CorrectAnswer: fit
  AnswerTests: omnitest(correctExpr='fit')
  Hint: Type "fit" at the R prompt.

- Class: mult_question
  Output: Choose the appropriate value for the estimated constant of
   the regression line.
  AnswerChoices: 29.148738; 0.003815; -12.41667; I haven't a clue
  CorrectAnswer: 29.148738
  AnswerTests: omnitest(correctVal='29.148738')
  Hint: Check the value under "Intercept".

- Class: text
  Output: R has extensive graphical and numerical tools to test if the
   assumptions for the regression are valid for this data set or are
   violated. We will first check the graphical tools. A basic
   assumption of linear regression is that the relationship is indeed
   linear. To check this, a very useful tool is the residual plot,
   which plots the residuals against the fitted value of the
   regression.

- Class: figure
  Output: On the right you can see residual plots, the residuals
   against the fitted values of the regression (the predicted
   values for the dependent variable). This plot shows if residuals
   have non-linear patterns. There could be a non-linear relationship
   between predictor variables and an outcome variable and the pattern
   could show up in this plot if the model doesn’t capture the
   non-linear relationship. If you find equally spread residuals around
   a horizontal line without distinct patterns, that is a good
   indication you don’t have non-linear relationships.
  Figure: figure_resfit.R
  FigureType: new

- Class: text
  Output: In the graph on the right we see residual plots
   from a "good" model and a "bad" model. The good model data are
   simulated in a way that meets the regression assumptions very well,
   while the bad model data are not. What do you think? Do you see
   differences between the two cases? I don’t see any distinctive
   pattern in Case 1, but I see a parabola in Case 2, where the
   non-linear relationship was not explained by the model and was left
   out in the residuals.

- Class: cmd_question
  Output: Let us now check the regression that we've just run. To produce
   this plot with R, we can use the plot() function. Giving a
   regression output object to the plot() function, it produces
   different regression diagnostics plots. To produce the residual
   plot, enter "plot(fit, which=1)". Do it now.
  CorrectAnswer: plot(fit, which=1)
  AnswerTests: omnitest(correctExpr='plot(fit, which=1)')
  Hint: Enter "plot(fit, which=1)" to check the residual plot.

- Class: mult_question
  Output: According to the residual plot that you just obtained
  AnswerChoices: There is a non-linear pattern with some outliers; There is a linear pattern with some outliers; There is a non-linear patterns with no outliers
  CorrectAnswer: There is a non-linear pattern with some outliers
  AnswerTests: omnitest(correctVal='There is a non-linear pattern with some outliers')
  Hint: If you check the residual plot, we see that the red line bends
   down, so there is a non-linear pattern. Furthermore, some points
   are marked as outliers.

- Class: text
  Output: The next assumption that we check is if the residuals seem
   to follow a normal distribution. For this we use a plot called the
   QQ plot. It plots the theoretical quantiles of the standard normal
   distribution in the horizontal axis, and the standardized residuals
   on the vertical axis. If the standardized residuals follow an exact
   standard normal distribution, we should observe all points in
   a straight line with a slope equal to 1.

- Class: figure
  Output: On the right, you can see two QQ plots.  Do residuals follow
   a straight line well or do they deviate severely? It’s good if
   residuals are lined well on the straight dashed line. What do you
   think? Of course they wouldn’t be a perfect straight line and this
   will be your call. Case 2 definitely concerns me. I would not be
   concerned by Case 1 too much, although an observation numbered as 38
   looks a little off.
  Figure: figure_QQ.R
  FigureType: new

- Class: cmd_question
  Output: We will now obtain the QQ plot for our data. Use the plot()
   function again with the "fit" object that contains our regression
   and give "which=2" as the second argument to obtain the QQ plot of
   the residuals.
  CorrectAnswer: plot(fit, which=2)
  AnswerTests: omnitest(correctExpr='plot(fit, which=2)')
  Hint: Enter "plot(fit, which=2)" to obtain the QQ plot of the
   residuals.

- Class: mult_question
  Output: According to what you see in the QQ plot for our regression
  AnswerChoices: The residuals look mostly normally distributed except for some outliers; The residuals do not look normally distributed at all; The residuals look mostly normally distributed and there are no outliers
  CorrectAnswer: The residuals look mostly normally distributed except for some outliers
  AnswerTests: omnitest(correctVal='The residuals look mostly normally distributed except for some outliers')
  Hint: Check the QQ plot, are all points mostly over the diagonal
   line? Are there any points departing form the diagonal line?

- Class: text
  Output: Another important part of the analysis is to identify
   influential observations. These are cases in our data that may
   have a big influence in our results, that is, if we exclude them
   from our data set, results may change significantly. Even though
   data have extreme values, they might not be influential to determine
   a regression line. That means, the results wouldn’t be much
   different if we either include or exclude them from analysis. They
   follow the trend in the majority of cases and they don’t really
   matter; they are not influential.

- Class: text
  Output: On the other hand, some cases could be very influential even
   if they look to be within a reasonable range of the values. They
   could be extreme cases against a regression line and can alter the
   results if we exclude them from analysis. Another way to put it is
   that they don’t get along with the trend in the majority of the
   cases.

- Class: figure
  Output: On the right we have plotted two cases. We watch out for
   outlying values at the upper right corner or at the lower right
   corner. Those spots are the places where cases can be influential
   against a regression line. Look for cases outside of a dashed line,
   Cook’s distance. When cases are outside of the Cook’s distance
   (meaning they have high Cook’s distance scores), the cases are
   influential to the regression results. The regression results will
   be altered if we exclude those cases.
  Figure: figure_lever.R
  FigureType: new

- Class: text
  Output: Case 1 is the typical look when there is no influential
   case, or cases. You can barely see Cook’s distance lines (a red
   dashed line) because all cases are well inside of the Cook’s
   distance lines. In Case 2, a case is far beyond the Cook’s distance
   lines (the other residuals appear clustered on the left because the
   second plot is scaled to show larger area than the first plot). The
   plot identified the influential observation as 49. If I exclude the
   49th case from the analysis, the slope coefficient changes from 2.14
   to 2.68 and R2 from .757 to .851. Pretty big impact!

- Class: cmd_question
  Output: We will first identify outliers in our regression. For this,
   we can plot the size of Cook's distance, which is a measure showing
   that a residual may be an outlier in the regression. To
   obtain this plot, use the plot() function again on the "fit" object
   that we created, but now give the argument "which=4".
  CorrectAnswer: plot(fit, which=4)
  AnswerTests: omnitest(correctExpr='plot(fit, which=4)')
  Hint: Enter "plot(fit, which=4)" to plot Cook's distance for all the
   cases in the data set.

- Class: cmd_question
  Output: To check if any of the outliers may be an influential
   observation, we compute another measure called leverage. Using now
   the plot() function on the "fit" object, giving "which=5" as
   argument, will show which cases are above some threshold values for
   the Cook's distance measure. Try is now.
  CorrectAnswer: plot(fit, which=5)
  AnswerTests: omnitest(correctExpr='plot(fit, which=5)')
  Hint: Enter "plot(fit, which=5)" to plot Cook's distance for all the
   cases in the data set.

- Class: text
  Output: As it can be seen the three problematic cases are
   represented again. They are closed to the threshold, but they do
   not have a large leverage, meaning that their influence on the
   regression may be reduced.

- Class: cmd_question
  Output: A final R command that is useful to get a final picture of
   problematic cases, is the influence.measures() function. Giving the
   "fit" object as an argument, different statistics are shown. The
   Coook's distance measure and the leverage measure are shown in the
   last two columns. The cases marked with a "*" may deserve further
   investigation. One should try to exclude these cases and see how
   much the results of the regression change. Try now the
   influence.measures() function.
  CorrectAnswer: influence.measures(fit)
  AnswerTests: omnitest(correctExpr='influence.measures(fit)')
  Hint: Enter "influence.measures(fit)" to obtain influence measures
   for the regression.

- Class: text
  Output: This is the end of this tutorial on diagnostic measures for
   the linear regression.
